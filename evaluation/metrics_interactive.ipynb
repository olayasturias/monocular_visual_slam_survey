{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo for Metrics\n",
    "\n",
    "* command line executables: see README.md\n",
    "* algorithm documentation: [metrics.py API & Algorithm Documentation](metrics.py_API_Documentation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...some modules and settings for this demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import glob\n",
    "from evo.tools import log\n",
    "from evo.core import metrics\n",
    "from evo.tools import file_interface\n",
    "from evo.core import sync\n",
    "log.configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.tools import plot\n",
    "from evo.tools.plot import PlotMode\n",
    "from evo.core.metrics import PoseRelation, Unit\n",
    "from evo.tools.settings import SETTINGS\n",
    "\n",
    "# temporarily override some package settings\n",
    "SETTINGS.plot_figsize = [6, 6]\n",
    "SETTINGS.plot_split = True\n",
    "SETTINGS.plot_usetex = False\n",
    "\n",
    "# magic plot configuration\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.tools import file_interface\n",
    "import numpy as np\n",
    "\n",
    "DESC = \"Combine KITTI poses and timestamps files to a TUM trajectory file\"\n",
    "\n",
    "\n",
    "def kitti_poses_and_timestamps_to_trajectory(poses_file, timestamp_file):\n",
    "    pose_path = file_interface.read_kitti_poses_file(poses_file)\n",
    "    raw_timestamps_mat = file_interface.csv_read_matrix(timestamp_file)\n",
    "    error_msg = (\"timestamp file must have one column of timestamps and same number of rows as the KITTI poses file\")\n",
    "    if len(raw_timestamps_mat) > 0 and len(raw_timestamps_mat[0]) != 1 or len(raw_timestamps_mat) != pose_path.num_poses:\n",
    "        raise file_interface.FileInterfaceException(error_msg)\n",
    "    try:\n",
    "        timestamps_mat = np.array(raw_timestamps_mat).astype(float)\n",
    "    except ValueError:\n",
    "        raise file_interface.FileInterfaceException(error_msg)\n",
    "    return PoseTrajectory3D(poses_se3=pose_path.poses_se3, timestamps=timestamps_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# interactive widgets configuration\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mipywidgets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m check_opts_ape \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39malign_origin\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\u001b[39m\"\u001b[39m\u001b[39malign\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mcorrect_scale\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mshow_plot\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[1;32m      5\u001b[0m check_boxes_ape\u001b[39m=\u001b[39m[ipywidgets\u001b[39m.\u001b[39mCheckbox(description\u001b[39m=\u001b[39mdesc, value\u001b[39m=\u001b[39mval) \u001b[39mfor\u001b[39;00m desc, val \u001b[39min\u001b[39;00m check_opts_ape\u001b[39m.\u001b[39mitems()]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "# interactive widgets configuration\n",
    "import ipywidgets\n",
    "\n",
    "check_opts_ape = {\"align_origin\": True,\"align\": False, \"correct_scale\": True, \"show_plot\": True}\n",
    "check_boxes_ape=[ipywidgets.Checkbox(description=desc, value=val) for desc, val in check_opts_ape.items()]\n",
    "check_opts_rpe = {\"align_origin\": True,\"align\": False, \"correct_scale\": True, \"all_pairs\": True, \"show_plot\": True}\n",
    "check_boxes_rpe=[ipywidgets.Checkbox(description=desc, value=val) for desc, val in check_opts_rpe.items()]\n",
    "delta_input = ipywidgets.FloatText(value=1.0, description='delta', disabled=False, color='black')\n",
    "delta_unit_selector=ipywidgets.Dropdown(\n",
    "    options={u.value: u for u in Unit if u is not Unit.seconds},\n",
    "    value=Unit.frames, description='delta_unit'\n",
    ")\n",
    "plotmode_selector=ipywidgets.Dropdown(\n",
    "    options={p.value: p for p in PlotMode},\n",
    "    value=PlotMode.xy, description='plot_mode'\n",
    ")\n",
    "pose_relation_selector=ipywidgets.Dropdown(\n",
    "    options={p.value: p for p in PoseRelation},\n",
    "    value=PoseRelation.translation_part, description='pose_relation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_dataframe(reference,estimate,algorithm,dataset,trajectory,correct_scale=True,correct_only_scale=False,n=-1,use_aligned_trajectories = True,t_offset = .1):\n",
    "    reference, estimate = sync.associate_trajectories(reference, estimate)\n",
    "    est = copy.deepcopy(estimate)\n",
    "    if use_aligned_trajectories:\n",
    "        est.align(reference, correct_scale=correct_scale, correct_only_scale=correct_only_scale, n=n)\n",
    "\n",
    "    if use_aligned_trajectories:\n",
    "        data = (reference, est) \n",
    "    else:\n",
    "        data = (reference, estimate)\n",
    "    \n",
    "    result_dict = {}\n",
    "    #APE_t\n",
    "    pose_relation = metrics.PoseRelation.translation_part\n",
    "    ape_metric = metrics.APE(pose_relation)\n",
    "    ape_metric.process_data(data)\n",
    "    ape_stats = ape_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'ape_t'):ape_stats})\n",
    "    #APE_rad\n",
    "    pose_relation = metrics.PoseRelation.rotation_angle_rad\n",
    "    ape_metric = metrics.APE(pose_relation)\n",
    "    ape_metric.process_data(data)\n",
    "    ape_stats = ape_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'ape_rad'):ape_stats})\n",
    "    \n",
    "    #RPE_t\n",
    "    pose_relation = metrics.PoseRelation.translation_part\n",
    "    rpe_metric = metrics.RPE(pose_relation)\n",
    "    rpe_metric.process_data(data)\n",
    "    rpe_stats = rpe_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'rpe_t'):rpe_stats})\n",
    "    #RPE_rad\n",
    "    pose_relation = metrics.PoseRelation.rotation_angle_rad\n",
    "    rpe_metric = metrics.RPE(pose_relation)\n",
    "    rpe_metric.process_data(data)\n",
    "    rpe_stats = rpe_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'rpe_rad'):rpe_stats})\n",
    "    \n",
    "    result_df = pd.DataFrame(result_dict)\n",
    "    return result_df\n",
    "\n",
    "def dso_estimate_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime  = reference.timestamps[-1]\n",
    "    estimate_aux = pd.read_csv(estimate, header = None, delim_whitespace = True, names = ['t', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'] )\n",
    "    len_estimate = len(estimate_aux['t'].tolist())\n",
    "    estimate_new_ts = np.linspace(startime*1e9,endtime*1e9, num = len_estimate)\n",
    "    estimate_aux['#timestamp'] = estimate_new_ts.flatten().tolist()\n",
    "    estimate_aux.to_csv('temp.txt', header = True, index = False, columns = ['#timestamp', 'x', 'y', 'z', 'qw', 'qx', 'qy', 'qz'])\n",
    "    estimate_converted = file_interface.read_euroc_csv_trajectory('temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_converted\n",
    "\n",
    "def dso_aqualoc_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime  = reference.timestamps[-1]\n",
    "    estimate_aux = pd.read_csv(estimate, header = None, delim_whitespace = True, names = ['t', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'] )\n",
    "    len_estimate = len(estimate_aux['t'].tolist())\n",
    "    estimate_new_ts = np.linspace(startime*1e9,endtime*1e9, num = len_estimate)\n",
    "    estimate_aux['#timestamp'] = estimate_new_ts.flatten().tolist()\n",
    "    \n",
    "    len_ref = reference.num_poses\n",
    "    print(len_estimate/len_ref)\n",
    "    if len_estimate/len_ref > 1:\n",
    "        estimate_aux = estimate_aux[estimate_aux.index % int(len_estimate/len_ref) == 0]  # Selects every 3rd raw starting from 0\n",
    "#         estimate_aux = estimate_aux.groupby(estimate_aux.index // int(len_estimate/len_ref)).min()\n",
    "#         estimate_aux.drop(index=estimate_aux.index[0], axis=0, inplace=True)\n",
    "    \n",
    "    estimate_aux.to_csv('temp.txt', header = True, index = False, columns = ['#timestamp', 'x', 'y', 'z', 'qw', 'qx', 'qy', 'qz'])\n",
    "    estimate_converted = file_interface.read_euroc_csv_trajectory('temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_converted   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degenerate covariance rank, Umeyama alignment is not possible\n",
      "failed to execute evaluation in  MIMIR OceanFloor/track1_light ../results/DSO/MIMIR/OceanFloor/track1_light/result_4.txt  with algorithm  DSO\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR SeaFloor/track0 ../results/ORB_SLAM3/MIMIR/SeaFloor/track0/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR SeaFloor/track1 ../results/ORB_SLAM3/MIMIR/SeaFloor/track1/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR SeaFloor/track2 ../results/ORB_SLAM3/MIMIR/SeaFloor/track2/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR SeaFloor_Algae/track0 ../results/ORB_SLAM3/MIMIR/SeaFloor_Algae/track0/papaer_4.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR SeaFloor_Algae/track1 ../results/ORB_SLAM3/MIMIR/SeaFloor_Algae/track1/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR SeaFloor_Algae/track2 ../results/ORB_SLAM3/MIMIR/SeaFloor_Algae/track2/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "Degenerate covariance rank, Umeyama alignment is not possible\n",
      "failed to execute evaluation in  MIMIR SandPipe/track0_light ../results/ORB_SLAM3/MIMIR/SandPipe/track0_light/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR OceanFloor/track0_light ../results/ORB_SLAM3/MIMIR/OceanFloor/track0_light/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR OceanFloor/track0_dark ../results/ORB_SLAM3/MIMIR/OceanFloor/track0_dark/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n",
      "found no matching timestamps between first trajectory and second trajectory with max. time diff 0.01 (s) and time offset 0.0 (s)\n",
      "failed to execute evaluation in  MIMIR OceanFloor/track1_light ../results/ORB_SLAM3/MIMIR/OceanFloor/track1_light/paper_2.txt  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">DSO</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">ORB_SLAM3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">MIMIR</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">MIMIR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">SeaFloor/track1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">SandPipe/track0_dark</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>...</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>9.065574</td>\n",
       "      <td>1.959864</td>\n",
       "      <td>1.408433</td>\n",
       "      <td>0.059295</td>\n",
       "      <td>11.689322</td>\n",
       "      <td>2.210011</td>\n",
       "      <td>1.329334</td>\n",
       "      <td>0.070791</td>\n",
       "      <td>13.902026</td>\n",
       "      <td>1.918789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184804</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>20.063468</td>\n",
       "      <td>2.169236</td>\n",
       "      <td>0.184527</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>20.102529</td>\n",
       "      <td>2.175899</td>\n",
       "      <td>0.185045</td>\n",
       "      <td>0.006746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.643014</td>\n",
       "      <td>1.937938</td>\n",
       "      <td>0.572582</td>\n",
       "      <td>0.049979</td>\n",
       "      <td>10.729307</td>\n",
       "      <td>2.159498</td>\n",
       "      <td>0.579303</td>\n",
       "      <td>0.058518</td>\n",
       "      <td>12.634090</td>\n",
       "      <td>1.887805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175953</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>18.620537</td>\n",
       "      <td>2.048036</td>\n",
       "      <td>0.175708</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>18.698295</td>\n",
       "      <td>2.052188</td>\n",
       "      <td>0.175863</td>\n",
       "      <td>0.005542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>7.370074</td>\n",
       "      <td>1.968481</td>\n",
       "      <td>0.459433</td>\n",
       "      <td>0.047346</td>\n",
       "      <td>10.577913</td>\n",
       "      <td>2.175257</td>\n",
       "      <td>0.462228</td>\n",
       "      <td>0.049869</td>\n",
       "      <td>11.561369</td>\n",
       "      <td>1.934774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166353</td>\n",
       "      <td>0.004647</td>\n",
       "      <td>18.012820</td>\n",
       "      <td>1.962674</td>\n",
       "      <td>0.166802</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>18.151291</td>\n",
       "      <td>1.959524</td>\n",
       "      <td>0.167937</td>\n",
       "      <td>0.004770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.875342</td>\n",
       "      <td>0.292342</td>\n",
       "      <td>1.286792</td>\n",
       "      <td>0.031907</td>\n",
       "      <td>4.639204</td>\n",
       "      <td>0.469805</td>\n",
       "      <td>1.196468</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>5.800524</td>\n",
       "      <td>0.343431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056506</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>7.471170</td>\n",
       "      <td>0.714939</td>\n",
       "      <td>0.056364</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>7.381426</td>\n",
       "      <td>0.723230</td>\n",
       "      <td>0.057566</td>\n",
       "      <td>0.003847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.975683</td>\n",
       "      <td>1.109015</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>3.053090</td>\n",
       "      <td>0.960783</td>\n",
       "      <td>0.074542</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>4.650059</td>\n",
       "      <td>1.050165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>8.368200</td>\n",
       "      <td>1.031750</td>\n",
       "      <td>0.029929</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>8.178519</td>\n",
       "      <td>1.007844</td>\n",
       "      <td>0.033890</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>68.654540</td>\n",
       "      <td>2.696202</td>\n",
       "      <td>26.751720</td>\n",
       "      <td>0.189359</td>\n",
       "      <td>59.682626</td>\n",
       "      <td>3.122814</td>\n",
       "      <td>24.094187</td>\n",
       "      <td>0.221288</td>\n",
       "      <td>25.412874</td>\n",
       "      <td>2.416443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460660</td>\n",
       "      <td>0.078623</td>\n",
       "      <td>44.849542</td>\n",
       "      <td>3.140994</td>\n",
       "      <td>0.475021</td>\n",
       "      <td>0.073628</td>\n",
       "      <td>44.854866</td>\n",
       "      <td>3.140915</td>\n",
       "      <td>0.489891</td>\n",
       "      <td>0.081986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sse</th>\n",
       "      <td>45694.651980</td>\n",
       "      <td>2135.634042</td>\n",
       "      <td>1100.943768</td>\n",
       "      <td>1.951349</td>\n",
       "      <td>75562.058348</td>\n",
       "      <td>2700.933361</td>\n",
       "      <td>975.454922</td>\n",
       "      <td>2.766285</td>\n",
       "      <td>105716.673927</td>\n",
       "      <td>2013.918042</td>\n",
       "      <td>...</td>\n",
       "      <td>57.990677</td>\n",
       "      <td>0.075780</td>\n",
       "      <td>679894.725835</td>\n",
       "      <td>7947.735830</td>\n",
       "      <td>57.476598</td>\n",
       "      <td>0.073642</td>\n",
       "      <td>689010.389453</td>\n",
       "      <td>8072.387384</td>\n",
       "      <td>58.347582</td>\n",
       "      <td>0.077551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DSO                                                    \\\n",
       "                 MIMIR                                                     \n",
       "       SeaFloor/track1                                                     \n",
       "                 ape_t      ape_rad        rpe_t   rpe_rad         ape_t   \n",
       "rmse          9.065574     1.959864     1.408433  0.059295     11.689322   \n",
       "mean          7.643014     1.937938     0.572582  0.049979     10.729307   \n",
       "median        7.370074     1.968481     0.459433  0.047346     10.577913   \n",
       "std           4.875342     0.292342     1.286792  0.031907      4.639204   \n",
       "min           0.975683     1.109015     0.129949  0.002997      3.053090   \n",
       "max          68.654540     2.696202    26.751720  0.189359     59.682626   \n",
       "sse       45694.651980  2135.634042  1100.943768  1.951349  75562.058348   \n",
       "\n",
       "                                                                       ...  \\\n",
       "                                                                       ...   \n",
       "                                                                       ...   \n",
       "            ape_rad       rpe_t   rpe_rad          ape_t      ape_rad  ...   \n",
       "rmse       2.210011    1.329334  0.070791      13.902026     1.918789  ...   \n",
       "mean       2.159498    0.579303  0.058518      12.634090     1.887805  ...   \n",
       "median     2.175257    0.462228  0.049869      11.561369     1.934774  ...   \n",
       "std        0.469805    1.196468  0.039837       5.800524     0.343431  ...   \n",
       "min        0.960783    0.074542  0.000547       4.650059     1.050165  ...   \n",
       "max        3.122814   24.094187  0.221288      25.412874     2.416443  ...   \n",
       "sse     2700.933361  975.454922  2.766285  105716.673927  2013.918042  ...   \n",
       "\n",
       "                  ORB_SLAM3                                                   \\\n",
       "                      MIMIR                                                    \n",
       "       SandPipe/track0_dark                                                    \n",
       "                      rpe_t   rpe_rad          ape_t      ape_rad      rpe_t   \n",
       "rmse               0.184804  0.006680      20.063468     2.169236   0.184527   \n",
       "mean               0.175953  0.005482      18.620537     2.048036   0.175708   \n",
       "median             0.166353  0.004647      18.012820     1.962674   0.166802   \n",
       "std                0.056506  0.003819       7.471170     0.714939   0.056364   \n",
       "min                0.002960  0.000272       8.368200     1.031750   0.029929   \n",
       "max                0.460660  0.078623      44.849542     3.140994   0.475021   \n",
       "sse               57.990677  0.075780  679894.725835  7947.735830  57.476598   \n",
       "\n",
       "                                                                   \n",
       "                                                                   \n",
       "                                                                   \n",
       "         rpe_rad          ape_t      ape_rad      rpe_t   rpe_rad  \n",
       "rmse    0.006605      20.102529     2.175899   0.185045  0.006746  \n",
       "mean    0.005459      18.698295     2.052188   0.175863  0.005542  \n",
       "median  0.004680      18.151291     1.959524   0.167937  0.004770  \n",
       "std     0.003718       7.381426     0.723230   0.057566  0.003847  \n",
       "min     0.000252       8.178519     1.007844   0.033890  0.000252  \n",
       "max     0.073628      44.854866     3.140915   0.489891  0.081986  \n",
       "sse     0.073642  689010.389453  8072.387384  58.347582  0.077551  \n",
       "\n",
       "[7 rows x 216 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "result_df = pd.DataFrame() \n",
    "results_dict = {\"algorithm\": [\"DSO\", \"ORB_SLAM3\"],\n",
    "                \"dataset\": [\"MIMIR\"],\n",
    "                \"MIMIR\":[\"SeaFloor/track0\",\"SeaFloor/track1\",\"SeaFloor/track2\",\n",
    "                \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\",\"SeaFloor_Algae/track2\",\n",
    "                \"SandPipe/track0_light\",\"SandPipe/track0_dark\",\n",
    "                \"OceanFloor/track0_light\",\"OceanFloor/track0_dark\",\"OceanFloor/track1_light\",\"OceanFloor/track1_dark\"]}\n",
    "\n",
    "for algorithm in results_dict[\"algorithm\"]:\n",
    "    for dataset in results_dict[\"dataset\"]:\n",
    "        for track in results_dict[dataset]:\n",
    "            try:\n",
    "                # find files\n",
    "                result_dir = os.path.join('..','results',algorithm,dataset,track)\n",
    "                result_list = glob.glob(result_dir+\"/*.txt\")+glob.glob(result_dir+\"/*.tum\")\n",
    "                for result_file in result_list:\n",
    "                    gt_file = os.path.join('..','groundtruths',dataset,track,'data.tum')            \n",
    "                    #read GT with EVO\n",
    "                    gt_evo = file_interface.read_tum_trajectory_file(gt_file)\n",
    "                    #reat estimate with EVO\n",
    "                    if algorithm == \"DSO\":\n",
    "                        estimated_traj = dso_estimate_workaround(gt_evo, result_file)\n",
    "                    else:\n",
    "                        estimated_traj = file_interface.read_tum_trajectory_file(result_file)\n",
    "                    # save to dataframe\n",
    "                    df = metrics_dataframe(gt_evo,estimated_traj,algorithm,dataset,track)\n",
    "                    result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "            \n",
    "\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(\"failed to execute evaluation in \", dataset, track,result_file, \" with algorithm \", algorithm)\n",
    "                print(\"Check if results file doesnt exist, or it exists and its empty\")\n",
    "                print(\"---------------------------------------------------------------------------\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/dso/result/euroc/\"+trajectory+\"/result.txt\"\n",
    "\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except Exception as e: \n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  10\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"KITTI\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/dso/result/KITTI/\"+trajectory+\"/result.txt\"\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUM RGBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  rgbd_dataset_freiburg1_rpy\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    est_file = os.getcwd()+\"/results/dso/result/tum/\"+trajectory+\"/result.txt\"\n",
    "    try:\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed aligning trajectory, trying without align archaeo_sequence_2_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_3_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_4_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_5_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_6_raw_data\n",
      "succeeded\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ORB_SLAM3 Mono\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"KITTI\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    \n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/orb-slam/kf_KITTI_\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/orb-slam/f_dataset-\"+trajectory+\"_mono.txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        est.timestamps /= 1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  rgbd_dataset_freiburg3_nostructure_notexture_far\n"
     ]
    }
   ],
   "source": [
    "dataset = \"TUM_RGBD\"\n",
    "algorithm = \"ORB_SLAM3\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    est_file = os.getcwd()+\"/results/orb-slam/kf_\"+trajectory+\".txt\"\n",
    "    try:\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed in trajectory archaeo_sequence_4_raw_data\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    \n",
    "    tsdataframe = pd.read_csv(os.path.join(\"/home/olaya/Datasets/Aqualoc/Archaeological_site_sequences\",trajectory_folder,\"raw_data\",\"img_sequence_\"+trajectory+\".csv\"))\n",
    "#     print(tsdataframe)\n",
    "    for i,t in enumerate(ref.timestamps):\n",
    "        try:\n",
    "            ref.timestamps[i] = tsdataframe.iloc[int(t)][\"#timestamp [ns]\"]\n",
    "        except:\n",
    "            print(t)\n",
    "        \n",
    "\n",
    "    try: \n",
    "        est_file =  os.getcwd()+\"/results/orb-slam/kf_\"+trajectory_folder+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        ref.timestamps *= est.timestamps[-1]/ref.timestamps[-1]\n",
    "    #     print(est.timestamps[0],ref.timestamps[0])\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print('failed in trajectory', trajectory_folder)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TrianFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trianflow_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime = reference.timestamps[-1]\n",
    "    est = file_interface.read_kitti_poses_file(estimate)\n",
    "    np.savetxt('temp.txt',np.linspace(startime,endtime,est.num_poses))\n",
    "\n",
    "    estimate_fixed = kitti_poses_and_timestamps_to_trajectory(estimate, 'temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuRoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/euroc/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/tum/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI dataset\n",
    "**Load KITTI files with** entries of the first three rows of  SE(3)  matrices per line (no timestamps)\n",
    "DFVO records the dataset in KITTI format but with indexes, uncompatible with EVO. Therefore we create a temp file with valid format to load trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"KITTI\"\n",
    "algorithm = \"TRIANFLOW\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/kitti/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try: \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/aqualoc/Archaeological_site_sequences/\"+trajectory_folder+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print('failed in trajectory', trajectory_folder)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"MIMIR\"\n",
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/mimir\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"auv0\",\"pose_groundtruth\",\"data.tum\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/mimir/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DF-VO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileInterfaceException",
     "evalue": "csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/euroc/MH_01_easy/mav0/state_groundtruth_estimate0/data.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileInterfaceException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m abs_dataset_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/groundtruths/euroc\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;66;03m# Edit this to path of dataset w.r.t. current file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ref_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(abs_dataset_path,trajectory,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmav0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_groundtruth_estimate0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m ref \u001b[38;5;241m=\u001b[39m \u001b[43mfile_interface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_euroc_csv_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     est_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results/df-vo/result/euroc/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:196\u001b[0m, in \u001b[0;36mread_euroc_csv_trajectory\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_euroc_csv_trajectory\u001b[39m(file_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoseTrajectory3D:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    parses ground truth trajectory from EuRoC MAV state estimate .csv\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    :param file_path: <sequence>/mav0/state_groundtruth_estimate0/data.csv\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    :return: trajectory.PoseTrajectory3D object\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     raw_mat \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_read_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuRoC format ground truth must have at least 8 entries per row \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand no trailing delimiter at the end of the rows (comma)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_mat \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(raw_mat) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_mat[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:84\u001b[0m, in \u001b[0;36mcsv_read_matrix\u001b[0;34m(file_path, delim, comment_str)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileInterfaceException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv file \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_path) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     85\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     skip_3_bytes \u001b[38;5;241m=\u001b[39m has_utf8_bom(file_path)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileInterfaceException\u001b[0m: csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/euroc/MH_01_easy/mav0/state_groundtruth_estimate0/data.csv does not exist"
     ]
    }
   ],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/euroc/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        est.timestamps /= 1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileInterfaceException",
     "evalue": "csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/tum_rgbd/rgbd_dataset_freiburg3_nostructure_notexture_far/groundtruth.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileInterfaceException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m abs_dataset_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/groundtruths/tum_rgbd\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;66;03m# Edit this to path of dataset w.r.t. current file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ref_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(abs_dataset_path,trajectory,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundtruth.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m ref \u001b[38;5;241m=\u001b[39m \u001b[43mfile_interface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_tum_trajectory_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \n\u001b[1;32m     10\u001b[0m     est_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results/df-vo/result/tum/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:103\u001b[0m, in \u001b[0;36mread_tum_trajectory_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_tum_trajectory_file\u001b[39m(file_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoseTrajectory3D:\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    parses trajectory file in TUM format (timestamp tx ty tz qx qy qz qw)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    :param file_path: the trajectory file path (or file handle)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    :return: trajectory.PoseTrajectory3D object\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     raw_mat \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_read_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTUM trajectory files must have 8 entries per row \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand no trailing delimiter at the end of the rows (space)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_mat \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(raw_mat) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_mat[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:84\u001b[0m, in \u001b[0;36mcsv_read_matrix\u001b[0;34m(file_path, delim, comment_str)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileInterfaceException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv file \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_path) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     85\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     skip_3_bytes \u001b[38;5;241m=\u001b[39m has_utf8_bom(file_path)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileInterfaceException\u001b[0m: csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/tum_rgbd/rgbd_dataset_freiburg3_nostructure_notexture_far/groundtruth.txt does not exist"
     ]
    }
   ],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg1_360\",\"rgbd_dataset_freiburg1_rpy\", \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/tum/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI dataset\n",
    "**Load KITTI files with** entries of the first three rows of  SE(3)  matrices per line (no timestamps)\n",
    "DFVO records the dataset in KITTI format but with indexes, uncompatible with EVO. Therefore we create a temp file with valid format to load trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"KITTI\"\n",
    "algorithm = \"DFVO\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/kitti/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "\n",
    "        # create temporary file\n",
    "        temp_est_file = pd.read_csv(est_file, header = None, sep=\" \", index_col=0)\n",
    "        temp_est_file.to_csv(\"temp.txt\",sep=\" \",index=False, header= False)\n",
    "\n",
    "\n",
    "        est_fixed = kitti_poses_and_timestamps_to_trajectory(\"temp.txt\", timestamp_file) \n",
    "\n",
    "        df = metrics_dataframe(ref,est_fixed,algorithm,dataset,trajectory)    \n",
    "        #delete temporary file\n",
    "        os.remove(\"temp.txt\")\n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)      \n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/aqualoc/Archaeological_site_sequences/\"+trajectory_folder+\"/\"+trajectory_folder+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"MIMIR\"\n",
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/mimir\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"auv0\",\"pose_groundtruth\",\"data.tum\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/mimir/\"+trajectory+\"/result.txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        ref.timestamps*=1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-6b8b1023934c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-6b8b1023934c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    trajectories = [,,\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "trajectories = [\"\",\"\",\"\", \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16958469459171965"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.DFVO.MIMIR[\"OceanFloor/track0_dark\"].rpe_rad.loc[\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evo.tools import pandas_bridge\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for result in results:\n",
    "    df = pd.concat((df, pandas_bridge.result_to_df(result)), axis=\"columns\")\n",
    "df = df.T\n",
    "df.loc[(df['info'].dataset == \"TUM_RGBD\")  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(df.info.dataset == \"KITTI\") & (df.info.algorithm == \"DF-VO\")]\n",
    "df.loc[(df['info'].dataset == \"TUM_RGBD\") & (df['info'].algorithm == \"ORB_SLAM3_mono\") & (df['info'].trajectory == \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\")][\"stats\"][\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['info'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d27047251d9f5c816fed1496de9dc0fee70169f6d1b553282d190c74f3af7a3d"
   }
  },
  "widgets": {
   "state": {
    "54cc6cb1d20f45438fb3663d98d29406": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "88c180f9f59147a592d738936cecf614": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
