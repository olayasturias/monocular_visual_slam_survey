{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo for Metrics\n",
    "\n",
    "* command line executables: see README.md\n",
    "* algorithm documentation: [metrics.py API & Algorithm Documentation](metrics.py_API_Documentation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...some modules and settings for this demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import copy\n",
    "from evo.tools import log\n",
    "from evo.core import metrics\n",
    "from evo.tools import file_interface\n",
    "from evo.core import sync\n",
    "log.configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.tools import plot\n",
    "from evo.tools.plot import PlotMode\n",
    "from evo.core.metrics import PoseRelation, Unit\n",
    "from evo.tools.settings import SETTINGS\n",
    "\n",
    "# temporarily override some package settings\n",
    "SETTINGS.plot_figsize = [6, 6]\n",
    "SETTINGS.plot_split = True\n",
    "SETTINGS.plot_usetex = False\n",
    "\n",
    "# magic plot configuration\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.tools import file_interface\n",
    "import numpy as np\n",
    "\n",
    "DESC = \"Combine KITTI poses and timestamps files to a TUM trajectory file\"\n",
    "\n",
    "\n",
    "def kitti_poses_and_timestamps_to_trajectory(poses_file, timestamp_file):\n",
    "    pose_path = file_interface.read_kitti_poses_file(poses_file)\n",
    "    raw_timestamps_mat = file_interface.csv_read_matrix(timestamp_file)\n",
    "    error_msg = (\"timestamp file must have one column of timestamps and same number of rows as the KITTI poses file\")\n",
    "    if len(raw_timestamps_mat) > 0 and len(raw_timestamps_mat[0]) != 1 or len(raw_timestamps_mat) != pose_path.num_poses:\n",
    "        raise file_interface.FileInterfaceException(error_msg)\n",
    "    try:\n",
    "        timestamps_mat = np.array(raw_timestamps_mat).astype(float)\n",
    "    except ValueError:\n",
    "        raise file_interface.FileInterfaceException(error_msg)\n",
    "    return PoseTrajectory3D(poses_se3=pose_path.poses_se3, timestamps=timestamps_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive widgets configuration\n",
    "import ipywidgets\n",
    "\n",
    "check_opts_ape = {\"align_origin\": True,\"align\": False, \"correct_scale\": True, \"show_plot\": True}\n",
    "check_boxes_ape=[ipywidgets.Checkbox(description=desc, value=val) for desc, val in check_opts_ape.items()]\n",
    "check_opts_rpe = {\"align_origin\": True,\"align\": False, \"correct_scale\": True, \"all_pairs\": True, \"show_plot\": True}\n",
    "check_boxes_rpe=[ipywidgets.Checkbox(description=desc, value=val) for desc, val in check_opts_rpe.items()]\n",
    "delta_input = ipywidgets.FloatText(value=1.0, description='delta', disabled=False, color='black')\n",
    "delta_unit_selector=ipywidgets.Dropdown(\n",
    "    options={u.value: u for u in Unit if u is not Unit.seconds},\n",
    "    value=Unit.frames, description='delta_unit'\n",
    ")\n",
    "plotmode_selector=ipywidgets.Dropdown(\n",
    "    options={p.value: p for p in PlotMode},\n",
    "    value=PlotMode.xy, description='plot_mode'\n",
    ")\n",
    "pose_relation_selector=ipywidgets.Dropdown(\n",
    "    options={p.value: p for p in PoseRelation},\n",
    "    value=PoseRelation.translation_part, description='pose_relation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_dataframe(reference,estimate,algorithm,dataset,trajectory,correct_scale=True,correct_only_scale=False,n=-1,use_aligned_trajectories = True,t_offset = .1):\n",
    "    reference, estimate = sync.associate_trajectories(reference, estimate)\n",
    "    est = copy.deepcopy(estimate)\n",
    "    if use_aligned_trajectories:\n",
    "        est.align(reference, correct_scale=correct_scale, correct_only_scale=correct_only_scale, n=n)\n",
    "\n",
    "    if use_aligned_trajectories:\n",
    "        data = (reference, est) \n",
    "    else:\n",
    "        data = (reference, estimate)\n",
    "    \n",
    "    result_dict = {}\n",
    "    #APE_t\n",
    "    pose_relation = metrics.PoseRelation.translation_part\n",
    "    ape_metric = metrics.APE(pose_relation)\n",
    "    ape_metric.process_data(data)\n",
    "    ape_stats = ape_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'ape_t'):ape_stats})\n",
    "    #APE_rad\n",
    "    pose_relation = metrics.PoseRelation.rotation_angle_rad\n",
    "    ape_metric = metrics.APE(pose_relation)\n",
    "    ape_metric.process_data(data)\n",
    "    ape_stats = ape_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'ape_rad'):ape_stats})\n",
    "    \n",
    "    #RPE_t\n",
    "    pose_relation = metrics.PoseRelation.translation_part\n",
    "    rpe_metric = metrics.RPE(pose_relation)\n",
    "    rpe_metric.process_data(data)\n",
    "    rpe_stats = rpe_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'rpe_t'):rpe_stats})\n",
    "    #RPE_rad\n",
    "    pose_relation = metrics.PoseRelation.rotation_angle_rad\n",
    "    rpe_metric = metrics.RPE(pose_relation)\n",
    "    rpe_metric.process_data(data)\n",
    "    rpe_stats = rpe_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'rpe_rad'):rpe_stats})\n",
    "    \n",
    "    result_df = pd.DataFrame(result_dict)\n",
    "    return result_df\n",
    "\n",
    "def dso_estimate_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime  = reference.timestamps[-1]\n",
    "    estimate_aux = pd.read_csv(estimate, header = None, delim_whitespace = True, names = ['t', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'] )\n",
    "    len_estimate = len(estimate_aux['t'].tolist())\n",
    "    estimate_new_ts = np.linspace(startime*1e9,endtime*1e9, num = len_estimate)\n",
    "    estimate_aux['#timestamp'] = estimate_new_ts.flatten().tolist()\n",
    "    estimate_aux.to_csv('temp.txt', header = True, index = False, columns = ['#timestamp', 'x', 'y', 'z', 'qw', 'qx', 'qy', 'qz'])\n",
    "    estimate_converted = file_interface.read_euroc_csv_trajectory('temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_converted\n",
    "\n",
    "def dso_aqualoc_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime  = reference.timestamps[-1]\n",
    "    estimate_aux = pd.read_csv(estimate, header = None, delim_whitespace = True, names = ['t', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'] )\n",
    "    len_estimate = len(estimate_aux['t'].tolist())\n",
    "    estimate_new_ts = np.linspace(startime*1e9,endtime*1e9, num = len_estimate)\n",
    "    estimate_aux['#timestamp'] = estimate_new_ts.flatten().tolist()\n",
    "    \n",
    "    len_ref = reference.num_poses\n",
    "    print(len_estimate/len_ref)\n",
    "    if len_estimate/len_ref > 1:\n",
    "        estimate_aux = estimate_aux[estimate_aux.index % int(len_estimate/len_ref) == 0]  # Selects every 3rd raw starting from 0\n",
    "#         estimate_aux = estimate_aux.groupby(estimate_aux.index // int(len_estimate/len_ref)).min()\n",
    "#         estimate_aux.drop(index=estimate_aux.index[0], axis=0, inplace=True)\n",
    "    \n",
    "    estimate_aux.to_csv('temp.txt', header = True, index = False, columns = ['#timestamp', 'x', 'y', 'z', 'qw', 'qx', 'qy', 'qz'])\n",
    "    estimate_converted = file_interface.read_euroc_csv_trajectory('temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_converted   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "result_df = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/dso/result/euroc/\"+trajectory+\"/result.txt\"\n",
    "\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except Exception as e: \n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  10\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"KITTI\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/dso/result/KITTI/\"+trajectory+\"/result.txt\"\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUM RGBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  rgbd_dataset_freiburg1_rpy\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    est_file = os.getcwd()+\"/results/dso/result/tum/\"+trajectory+\"/result.txt\"\n",
    "    try:\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed aligning trajectory, trying without align archaeo_sequence_2_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_3_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_4_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_5_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_6_raw_data\n",
      "succeeded\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ORB_SLAM3 Mono\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"KITTI\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    \n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/orb-slam/kf_KITTI_\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/orb-slam/f_dataset-\"+trajectory+\"_mono.txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        est.timestamps /= 1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  rgbd_dataset_freiburg3_nostructure_notexture_far\n"
     ]
    }
   ],
   "source": [
    "dataset = \"TUM_RGBD\"\n",
    "algorithm = \"ORB_SLAM3\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    est_file = os.getcwd()+\"/results/orb-slam/kf_\"+trajectory+\".txt\"\n",
    "    try:\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed in trajectory archaeo_sequence_4_raw_data\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    \n",
    "    tsdataframe = pd.read_csv(os.path.join(\"/home/olaya/Datasets/Aqualoc/Archaeological_site_sequences\",trajectory_folder,\"raw_data\",\"img_sequence_\"+trajectory+\".csv\"))\n",
    "#     print(tsdataframe)\n",
    "    for i,t in enumerate(ref.timestamps):\n",
    "        try:\n",
    "            ref.timestamps[i] = tsdataframe.iloc[int(t)][\"#timestamp [ns]\"]\n",
    "        except:\n",
    "            print(t)\n",
    "        \n",
    "\n",
    "    try: \n",
    "        est_file =  os.getcwd()+\"/results/orb-slam/kf_\"+trajectory_folder+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        ref.timestamps *= est.timestamps[-1]/ref.timestamps[-1]\n",
    "    #     print(est.timestamps[0],ref.timestamps[0])\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print('failed in trajectory', trajectory_folder)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TrianFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trianflow_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime = reference.timestamps[-1]\n",
    "    est = file_interface.read_kitti_poses_file(estimate)\n",
    "    np.savetxt('temp.txt',np.linspace(startime,endtime,est.num_poses))\n",
    "\n",
    "    estimate_fixed = kitti_poses_and_timestamps_to_trajectory(estimate, 'temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuRoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/euroc/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/tum/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI dataset\n",
    "**Load KITTI files with** entries of the first three rows of  SE(3)  matrices per line (no timestamps)\n",
    "DFVO records the dataset in KITTI format but with indexes, uncompatible with EVO. Therefore we create a temp file with valid format to load trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"KITTI\"\n",
    "algorithm = \"TRIANFLOW\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/kitti/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try: \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/aqualoc/Archaeological_site_sequences/\"+trajectory_folder+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print('failed in trajectory', trajectory_folder)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"MIMIR\"\n",
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/mimir\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"auv0\",\"pose_groundtruth\",\"data.tum\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/mimir/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DF-VO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  V2_03_difficult\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/euroc/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        est.timestamps /= 1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg1_360\",\"rgbd_dataset_freiburg1_rpy\", \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/tum/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI dataset\n",
    "**Load KITTI files with** entries of the first three rows of  SE(3)  matrices per line (no timestamps)\n",
    "DFVO records the dataset in KITTI format but with indexes, uncompatible with EVO. Therefore we create a temp file with valid format to load trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"KITTI\"\n",
    "algorithm = \"DFVO\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/kitti/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "\n",
    "        # create temporary file\n",
    "        temp_est_file = pd.read_csv(est_file, header = None, sep=\" \", index_col=0)\n",
    "        temp_est_file.to_csv(\"temp.txt\",sep=\" \",index=False, header= False)\n",
    "\n",
    "\n",
    "        est_fixed = kitti_poses_and_timestamps_to_trajectory(\"temp.txt\", timestamp_file) \n",
    "\n",
    "        df = metrics_dataframe(ref,est_fixed,algorithm,dataset,trajectory)    \n",
    "        #delete temporary file\n",
    "        os.remove(\"temp.txt\")\n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)      \n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/aqualoc/Archaeological_site_sequences/\"+trajectory_folder+\"/\"+trajectory_folder+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"MIMIR\"\n",
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/mimir\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"auv0\",\"pose_groundtruth\",\"data.tum\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/mimir/\"+trajectory+\"/result.txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        ref.timestamps*=1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-6b8b1023934c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-6b8b1023934c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    trajectories = [,,\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "trajectories = [\"\",\"\",\"\", \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16958469459171965"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.DFVO.MIMIR[\"OceanFloor/track0_dark\"].rpe_rad.loc[\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evo.tools import pandas_bridge\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for result in results:\n",
    "    df = pd.concat((df, pandas_bridge.result_to_df(result)), axis=\"columns\")\n",
    "df = df.T\n",
    "df.loc[(df['info'].dataset == \"TUM_RGBD\")  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(df.info.dataset == \"KITTI\") & (df.info.algorithm == \"DF-VO\")]\n",
    "df.loc[(df['info'].dataset == \"TUM_RGBD\") & (df['info'].algorithm == \"ORB_SLAM3_mono\") & (df['info'].trajectory == \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\")][\"stats\"][\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['info'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "state": {
    "54cc6cb1d20f45438fb3663d98d29406": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "88c180f9f59147a592d738936cecf614": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
