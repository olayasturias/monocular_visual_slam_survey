{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo for Metrics\n",
    "\n",
    "* command line executables: see README.md\n",
    "* algorithm documentation: [metrics.py API & Algorithm Documentation](metrics.py_API_Documentation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...some modules and settings for this demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import glob\n",
    "from evo.tools import log\n",
    "from evo.core import metrics\n",
    "from evo.tools import file_interface\n",
    "from evo.core import sync\n",
    "log.configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.tools import plot\n",
    "from evo.tools.plot import PlotMode\n",
    "from evo.core.metrics import PoseRelation, Unit\n",
    "from evo.tools.settings import SETTINGS\n",
    "\n",
    "# temporarily override some package settings\n",
    "SETTINGS.plot_figsize = [6, 6]\n",
    "SETTINGS.plot_split = True\n",
    "SETTINGS.plot_usetex = False\n",
    "\n",
    "# magic plot configuration\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.tools import file_interface\n",
    "import numpy as np\n",
    "\n",
    "DESC = \"Combine KITTI poses and timestamps files to a TUM trajectory file\"\n",
    "\n",
    "\n",
    "def kitti_poses_and_timestamps_to_trajectory(poses_file, timestamp_file):\n",
    "    pose_path = file_interface.read_kitti_poses_file(poses_file)\n",
    "    raw_timestamps_mat = file_interface.csv_read_matrix(timestamp_file)\n",
    "    error_msg = (\"timestamp file must have one column of timestamps and same number of rows as the KITTI poses file\")\n",
    "    if len(raw_timestamps_mat) > 0 and len(raw_timestamps_mat[0]) != 1 or len(raw_timestamps_mat) != pose_path.num_poses:\n",
    "        raise file_interface.FileInterfaceException(error_msg)\n",
    "    try:\n",
    "        timestamps_mat = np.array(raw_timestamps_mat).astype(float)\n",
    "    except ValueError:\n",
    "        raise file_interface.FileInterfaceException(error_msg)\n",
    "    return PoseTrajectory3D(poses_se3=pose_path.poses_se3, timestamps=timestamps_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive widgets configuration\n",
    "import ipywidgets\n",
    "\n",
    "check_opts_ape = {\"align_origin\": True,\"align\": False, \"correct_scale\": True, \"show_plot\": True}\n",
    "check_boxes_ape=[ipywidgets.Checkbox(description=desc, value=val) for desc, val in check_opts_ape.items()]\n",
    "check_opts_rpe = {\"align_origin\": True,\"align\": False, \"correct_scale\": True, \"all_pairs\": True, \"show_plot\": True}\n",
    "check_boxes_rpe=[ipywidgets.Checkbox(description=desc, value=val) for desc, val in check_opts_rpe.items()]\n",
    "delta_input = ipywidgets.FloatText(value=1.0, description='delta', disabled=False, color='black')\n",
    "delta_unit_selector=ipywidgets.Dropdown(\n",
    "    options={u.value: u for u in Unit if u is not Unit.seconds},\n",
    "    value=Unit.frames, description='delta_unit'\n",
    ")\n",
    "plotmode_selector=ipywidgets.Dropdown(\n",
    "    options={p.value: p for p in PlotMode},\n",
    "    value=PlotMode.xy, description='plot_mode'\n",
    ")\n",
    "pose_relation_selector=ipywidgets.Dropdown(\n",
    "    options={p.value: p for p in PoseRelation},\n",
    "    value=PoseRelation.translation_part, description='pose_relation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_dataframe(reference,estimate,algorithm,dataset,trajectory,correct_scale=True,correct_only_scale=False,n=-1,use_aligned_trajectories = True,t_offset = .1):\n",
    "    reference, estimate = sync.associate_trajectories(reference, estimate)\n",
    "    est = copy.deepcopy(estimate)\n",
    "    if use_aligned_trajectories:\n",
    "        est.align(reference, correct_scale=correct_scale, correct_only_scale=correct_only_scale, n=n)\n",
    "\n",
    "    if use_aligned_trajectories:\n",
    "        data = (reference, est) \n",
    "    else:\n",
    "        data = (reference, estimate)\n",
    "    \n",
    "    result_dict = {}\n",
    "    #APE_t\n",
    "    pose_relation = metrics.PoseRelation.translation_part\n",
    "    ape_metric = metrics.APE(pose_relation)\n",
    "    ape_metric.process_data(data)\n",
    "    ape_stats = ape_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'ape_t'):ape_stats})\n",
    "    #APE_rad\n",
    "    pose_relation = metrics.PoseRelation.rotation_angle_rad\n",
    "    ape_metric = metrics.APE(pose_relation)\n",
    "    ape_metric.process_data(data)\n",
    "    ape_stats = ape_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'ape_rad'):ape_stats})\n",
    "    \n",
    "    #RPE_t\n",
    "    pose_relation = metrics.PoseRelation.translation_part\n",
    "    rpe_metric = metrics.RPE(pose_relation)\n",
    "    rpe_metric.process_data(data)\n",
    "    rpe_stats = rpe_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'rpe_t'):rpe_stats})\n",
    "    #RPE_rad\n",
    "    pose_relation = metrics.PoseRelation.rotation_angle_rad\n",
    "    rpe_metric = metrics.RPE(pose_relation)\n",
    "    rpe_metric.process_data(data)\n",
    "    rpe_stats = rpe_metric.get_all_statistics()\n",
    "    result_dict.update({(algorithm, dataset,trajectory,'rpe_rad'):rpe_stats})\n",
    "    \n",
    "    result_df = pd.DataFrame(result_dict)\n",
    "    return result_df\n",
    "\n",
    "def dso_estimate_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime  = reference.timestamps[-1]\n",
    "    estimate_aux = pd.read_csv(estimate, header = None, delim_whitespace = True, names = ['t', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'] )\n",
    "    len_estimate = len(estimate_aux['t'].tolist())\n",
    "    estimate_new_ts = np.linspace(startime*1e9,endtime*1e9, num = len_estimate)\n",
    "    estimate_aux['#timestamp'] = estimate_new_ts.flatten().tolist()\n",
    "    estimate_aux.to_csv('temp.txt', header = True, index = False, columns = ['#timestamp', 'x', 'y', 'z', 'qw', 'qx', 'qy', 'qz'])\n",
    "    estimate_converted = file_interface.read_euroc_csv_trajectory('temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_converted\n",
    "\n",
    "def dso_aqualoc_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime  = reference.timestamps[-1]\n",
    "    estimate_aux = pd.read_csv(estimate, header = None, delim_whitespace = True, names = ['t', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'] )\n",
    "    len_estimate = len(estimate_aux['t'].tolist())\n",
    "    estimate_new_ts = np.linspace(startime*1e9,endtime*1e9, num = len_estimate)\n",
    "    estimate_aux['#timestamp'] = estimate_new_ts.flatten().tolist()\n",
    "    \n",
    "    len_ref = reference.num_poses\n",
    "    print(len_estimate/len_ref)\n",
    "    if len_estimate/len_ref > 1:\n",
    "        estimate_aux = estimate_aux[estimate_aux.index % int(len_estimate/len_ref) == 0]  # Selects every 3rd raw starting from 0\n",
    "#         estimate_aux = estimate_aux.groupby(estimate_aux.index // int(len_estimate/len_ref)).min()\n",
    "#         estimate_aux.drop(index=estimate_aux.index[0], axis=0, inplace=True)\n",
    "    \n",
    "    estimate_aux.to_csv('temp.txt', header = True, index = False, columns = ['#timestamp', 'x', 'y', 'z', 'qw', 'qx', 'qy', 'qz'])\n",
    "    estimate_converted = file_interface.read_euroc_csv_trajectory('temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_converted   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "result_df = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EuRoC format ground truth must have at least 8 entries per row and no trailing delimiter at the end of the rows (comma)\n",
      "failed to execute evaluation in  MIMIR SeaFloor/track0  with algorithm  DSO\n",
      "Check if results file doesnt exist, or it exists and its empty\n",
      "TUM trajectory files must have 8 entries per row and no trailing delimiter at the end of the rows (space)\n",
      "failed to execute evaluation in  MIMIR SeaFloor/track0  with algorithm  ORB_SLAM3\n",
      "Check if results file doesnt exist, or it exists and its empty\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">DSO</th>\n",
       "      <th colspan=\"17\" halign=\"left\">ORB_SLAM3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">MIMIR</th>\n",
       "      <th colspan=\"17\" halign=\"left\">MIMIR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">SeaFloor/track1</th>\n",
       "      <th colspan=\"13\" halign=\"left\">SeaFloor/track0</th>\n",
       "      <th colspan=\"4\" halign=\"left\">SeaFloor/track1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>...</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "      <th>ape_t</th>\n",
       "      <th>ape_rad</th>\n",
       "      <th>rpe_t</th>\n",
       "      <th>rpe_rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>14.337472</td>\n",
       "      <td>2.203070</td>\n",
       "      <td>0.491636</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.374209</td>\n",
       "      <td>1.565015</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.011037</td>\n",
       "      <td>3.674021</td>\n",
       "      <td>2.265925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.011037</td>\n",
       "      <td>3.674021</td>\n",
       "      <td>2.265925</td>\n",
       "      <td>0.723265</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>8.519855</td>\n",
       "      <td>2.336416</td>\n",
       "      <td>0.341077</td>\n",
       "      <td>0.035828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.088308</td>\n",
       "      <td>2.180128</td>\n",
       "      <td>0.437745</td>\n",
       "      <td>0.052549</td>\n",
       "      <td>0.328720</td>\n",
       "      <td>1.564925</td>\n",
       "      <td>0.284690</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>3.299922</td>\n",
       "      <td>2.263854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284690</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>3.299922</td>\n",
       "      <td>2.263854</td>\n",
       "      <td>0.406597</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>7.768399</td>\n",
       "      <td>2.334966</td>\n",
       "      <td>0.254741</td>\n",
       "      <td>0.027107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>13.029927</td>\n",
       "      <td>2.130635</td>\n",
       "      <td>0.439704</td>\n",
       "      <td>0.045393</td>\n",
       "      <td>0.347961</td>\n",
       "      <td>1.560399</td>\n",
       "      <td>0.283624</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>3.176007</td>\n",
       "      <td>2.257966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283624</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>3.176007</td>\n",
       "      <td>2.257966</td>\n",
       "      <td>0.232524</td>\n",
       "      <td>0.010711</td>\n",
       "      <td>6.838960</td>\n",
       "      <td>2.347155</td>\n",
       "      <td>0.197779</td>\n",
       "      <td>0.019413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.853144</td>\n",
       "      <td>0.317110</td>\n",
       "      <td>0.223797</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>0.178817</td>\n",
       "      <td>0.016803</td>\n",
       "      <td>0.118085</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>1.615222</td>\n",
       "      <td>0.096850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118085</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>1.615222</td>\n",
       "      <td>0.096850</td>\n",
       "      <td>0.598156</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>3.498556</td>\n",
       "      <td>0.082305</td>\n",
       "      <td>0.226805</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.883920</td>\n",
       "      <td>1.575560</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.030807</td>\n",
       "      <td>1.544065</td>\n",
       "      <td>0.023694</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.281832</td>\n",
       "      <td>2.109719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023694</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.281832</td>\n",
       "      <td>2.109719</td>\n",
       "      <td>0.064990</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.862993</td>\n",
       "      <td>2.148194</td>\n",
       "      <td>0.018521</td>\n",
       "      <td>0.001393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.899964</td>\n",
       "      <td>3.139260</td>\n",
       "      <td>3.283229</td>\n",
       "      <td>0.185663</td>\n",
       "      <td>0.765430</td>\n",
       "      <td>1.613436</td>\n",
       "      <td>0.673176</td>\n",
       "      <td>0.038222</td>\n",
       "      <td>10.126899</td>\n",
       "      <td>2.401451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673176</td>\n",
       "      <td>0.038222</td>\n",
       "      <td>10.126899</td>\n",
       "      <td>2.401451</td>\n",
       "      <td>4.110566</td>\n",
       "      <td>0.042332</td>\n",
       "      <td>15.485135</td>\n",
       "      <td>2.457496</td>\n",
       "      <td>1.729057</td>\n",
       "      <td>0.122906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sse</th>\n",
       "      <td>113470.835538</td>\n",
       "      <td>2679.141672</td>\n",
       "      <td>133.179838</td>\n",
       "      <td>2.234653</td>\n",
       "      <td>6.441496</td>\n",
       "      <td>112.666509</td>\n",
       "      <td>4.274651</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>1457.830419</td>\n",
       "      <td>554.516747</td>\n",
       "      <td>...</td>\n",
       "      <td>4.274651</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>1457.830419</td>\n",
       "      <td>554.516747</td>\n",
       "      <td>55.972972</td>\n",
       "      <td>0.018104</td>\n",
       "      <td>20324.618340</td>\n",
       "      <td>1528.475581</td>\n",
       "      <td>32.457055</td>\n",
       "      <td>0.358137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DSO                                          ORB_SLAM3  \\\n",
       "                 MIMIR                                              MIMIR   \n",
       "       SeaFloor/track1                                    SeaFloor/track0   \n",
       "                 ape_t      ape_rad       rpe_t   rpe_rad           ape_t   \n",
       "rmse         14.337472     2.203070    0.491636  0.063684        0.374209   \n",
       "mean         13.088308     2.180128    0.437745  0.052549        0.328720   \n",
       "median       13.029927     2.130635    0.439704  0.045393        0.347961   \n",
       "std           5.853144     0.317110    0.223797  0.035976        0.178817   \n",
       "min           3.883920     1.575560    0.031272  0.002106        0.030807   \n",
       "max          25.899964     3.139260    3.283229  0.185663        0.765430   \n",
       "sse      113470.835538  2679.141672  133.179838  2.234653        6.441496   \n",
       "\n",
       "                                                                 ...  \\\n",
       "                                                                 ...   \n",
       "                                                                 ...   \n",
       "           ape_rad     rpe_t   rpe_rad        ape_t     ape_rad  ...   \n",
       "rmse      1.565015  0.308208  0.011037     3.674021    2.265925  ...   \n",
       "mean      1.564925  0.284690  0.009431     3.299922    2.263854  ...   \n",
       "median    1.560399  0.283624  0.008983     3.176007    2.257966  ...   \n",
       "std       0.016803  0.118085  0.005734     1.615222    0.096850  ...   \n",
       "min       1.544065  0.023694  0.001121     0.281832    2.109719  ...   \n",
       "max       1.613436  0.673176  0.038222    10.126899    2.401451  ...   \n",
       "sse     112.666509  4.274651  0.005482  1457.830419  554.516747  ...   \n",
       "\n",
       "                                                                          \\\n",
       "                                                                           \n",
       "                                                                           \n",
       "           rpe_t   rpe_rad        ape_t     ape_rad      rpe_t   rpe_rad   \n",
       "rmse    0.308208  0.011037     3.674021    2.265925   0.723265  0.013008   \n",
       "mean    0.284690  0.009431     3.299922    2.263854   0.406597  0.011237   \n",
       "median  0.283624  0.008983     3.176007    2.257966   0.232524  0.010711   \n",
       "std     0.118085  0.005734     1.615222    0.096850   0.598156  0.006551   \n",
       "min     0.023694  0.001121     0.281832    2.109719   0.064990  0.001023   \n",
       "max     0.673176  0.038222    10.126899    2.401451   4.110566  0.042332   \n",
       "sse     4.274651  0.005482  1457.830419  554.516747  55.972972  0.018104   \n",
       "\n",
       "                                                          \n",
       "                                                          \n",
       "       SeaFloor/track1                                    \n",
       "                 ape_t      ape_rad      rpe_t   rpe_rad  \n",
       "rmse          8.519855     2.336416   0.341077  0.035828  \n",
       "mean          7.768399     2.334966   0.254741  0.027107  \n",
       "median        6.838960     2.347155   0.197779  0.019413  \n",
       "std           3.498556     0.082305   0.226805  0.023427  \n",
       "min           0.862993     2.148194   0.018521  0.001393  \n",
       "max          15.485135     2.457496   1.729057  0.122906  \n",
       "sse       20324.618340  1528.475581  32.457055  0.358137  \n",
       "\n",
       "[7 rows x 48 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict = {\"algorithm\": [\"DSO\", \"ORB_SLAM3\"],\n",
    "                \"dataset\": [\"MIMIR\"],\n",
    "                \"MIMIR\":[\"SeaFloor/track0\",\"SeaFloor/track1\"]}\n",
    "\n",
    "for algorithm in results_dict[\"algorithm\"]:\n",
    "    for dataset in results_dict[\"dataset\"]:\n",
    "        for track in results_dict[dataset]:\n",
    "            try:\n",
    "                # find files\n",
    "                result_dir = os.path.join('..','results',algorithm,dataset,track)\n",
    "                result_list = glob.glob(result_dir+\"/*.txt\")\n",
    "                for result_file in result_list:\n",
    "                    gt_file = os.path.join('..','groundtruths',dataset,track,'data.tum')            \n",
    "                    #read GT with EVO\n",
    "                    gt_evo = file_interface.read_tum_trajectory_file(gt_file)\n",
    "                    #reat estimate with EVO\n",
    "                    if algorithm == \"DSO\":\n",
    "                        estimated_traj = dso_estimate_workaround(gt_evo, result_file)\n",
    "                    else:\n",
    "                        estimated_traj = file_interface.read_tum_trajectory_file(result_file)\n",
    "                    # save to dataframe\n",
    "                    df = metrics_dataframe(gt_evo,estimated_traj,algorithm,dataset,track)\n",
    "                    result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "            \n",
    "\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(\"failed to execute evaluation in \", dataset, track, \" with algorithm \", algorithm)\n",
    "                print(\"Check if results file doesnt exist, or it exists and its empty\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/dso/result/euroc/\"+trajectory+\"/result.txt\"\n",
    "\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except Exception as e: \n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  10\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"KITTI\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/dso/result/KITTI/\"+trajectory+\"/result.txt\"\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUM RGBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  rgbd_dataset_freiburg1_rpy\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"DSO\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    est_file = os.getcwd()+\"/results/dso/result/tum/\"+trajectory+\"/result.txt\"\n",
    "    try:\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed aligning trajectory, trying without align archaeo_sequence_2_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_3_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_4_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_5_raw_data\n",
      "succeeded\n",
      "failed aligning trajectory, trying without align archaeo_sequence_6_raw_data\n",
      "succeeded\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ORB_SLAM3 Mono\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"KITTI\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    \n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/orb-slam/kf_KITTI_\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/orb-slam/f_dataset-\"+trajectory+\"_mono.txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        est.timestamps /= 1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed at trajectory  rgbd_dataset_freiburg3_nostructure_notexture_far\n"
     ]
    }
   ],
   "source": [
    "dataset = \"TUM_RGBD\"\n",
    "algorithm = \"ORB_SLAM3\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    est_file = os.getcwd()+\"/results/orb-slam/kf_\"+trajectory+\".txt\"\n",
    "    try:\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed in trajectory archaeo_sequence_4_raw_data\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"ORB_SLAM3\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    \n",
    "    tsdataframe = pd.read_csv(os.path.join(\"/home/olaya/Datasets/Aqualoc/Archaeological_site_sequences\",trajectory_folder,\"raw_data\",\"img_sequence_\"+trajectory+\".csv\"))\n",
    "#     print(tsdataframe)\n",
    "    for i,t in enumerate(ref.timestamps):\n",
    "        try:\n",
    "            ref.timestamps[i] = tsdataframe.iloc[int(t)][\"#timestamp [ns]\"]\n",
    "        except:\n",
    "            print(t)\n",
    "        \n",
    "\n",
    "    try: \n",
    "        est_file =  os.getcwd()+\"/results/orb-slam/kf_\"+trajectory_folder+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        ref.timestamps *= est.timestamps[-1]/ref.timestamps[-1]\n",
    "    #     print(est.timestamps[0],ref.timestamps[0])\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print('failed in trajectory', trajectory_folder)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TrianFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trianflow_workaround(reference, estimate):\n",
    "    startime = reference.timestamps[0]\n",
    "    endtime = reference.timestamps[-1]\n",
    "    est = file_interface.read_kitti_poses_file(estimate)\n",
    "    np.savetxt('temp.txt',np.linspace(startime,endtime,est.num_poses))\n",
    "\n",
    "    estimate_fixed = kitti_poses_and_timestamps_to_trajectory(estimate, 'temp.txt')\n",
    "    os.remove('temp.txt')\n",
    "    return estimate_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EuRoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/euroc/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg1_rpy\",\"rgbd_dataset_freiburg1_360\", \"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/tum/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI dataset\n",
    "**Load KITTI files with** entries of the first three rows of  SE(3)  matrices per line (no timestamps)\n",
    "DFVO records the dataset in KITTI format but with indexes, uncompatible with EVO. Therefore we create a temp file with valid format to load trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"KITTI\"\n",
    "algorithm = \"TRIANFLOW\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/kitti/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try: \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/aqualoc/Archaeological_site_sequences/\"+trajectory_folder+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = dso_estimate_workaround(ref, est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print('failed in trajectory', trajectory_folder)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"TRIANFLOW\"\n",
    "dataset = \"MIMIR\"\n",
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/mimir\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"auv0\",\"pose_groundtruth\",\"data.tum\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/trianflow/results/mimir/\"+trajectory+\"/trianflow_results.txt\"\n",
    "        estimate_fixed = trianflow_workaround(ref,est_file)\n",
    "        df = metrics_dataframe(ref,estimate_fixed,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DF-VO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuRoC dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileInterfaceException",
     "evalue": "csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/euroc/MH_01_easy/mav0/state_groundtruth_estimate0/data.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileInterfaceException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m abs_dataset_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/groundtruths/euroc\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;66;03m# Edit this to path of dataset w.r.t. current file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ref_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(abs_dataset_path,trajectory,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmav0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_groundtruth_estimate0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m ref \u001b[38;5;241m=\u001b[39m \u001b[43mfile_interface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_euroc_csv_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     est_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results/df-vo/result/euroc/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:196\u001b[0m, in \u001b[0;36mread_euroc_csv_trajectory\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_euroc_csv_trajectory\u001b[39m(file_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoseTrajectory3D:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    parses ground truth trajectory from EuRoC MAV state estimate .csv\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    :param file_path: <sequence>/mav0/state_groundtruth_estimate0/data.csv\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    :return: trajectory.PoseTrajectory3D object\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     raw_mat \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_read_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuRoC format ground truth must have at least 8 entries per row \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand no trailing delimiter at the end of the rows (comma)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_mat \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(raw_mat) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_mat[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:84\u001b[0m, in \u001b[0;36mcsv_read_matrix\u001b[0;34m(file_path, delim, comment_str)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileInterfaceException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv file \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_path) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     85\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     skip_3_bytes \u001b[38;5;241m=\u001b[39m has_utf8_bom(file_path)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileInterfaceException\u001b[0m: csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/euroc/MH_01_easy/mav0/state_groundtruth_estimate0/data.csv does not exist"
     ]
    }
   ],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"EUROC\"\n",
    "trajectories = [\"MH_01_easy\",\"MH_02_easy\", \"MH_03_medium\", \"MH_04_difficult\", \"MH_05_difficult\", \"V1_01_easy\", \"V1_02_medium\", \"V1_03_difficult\", \"V2_01_easy\", \"V2_02_medium\", \"V2_03_difficult\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/euroc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"mav0\",\"state_groundtruth_estimate0\",\"data.csv\")\n",
    "    ref = file_interface.read_euroc_csv_trajectory(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/euroc/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        est.timestamps /= 1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUM dataset\n",
    "**Load TUM files with** 3D position and orientation quaternion per line ($x$ $y$ $z$ $q_x$ $q_y$ $q_z$ $q_w$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileInterfaceException",
     "evalue": "csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/tum_rgbd/rgbd_dataset_freiburg3_nostructure_notexture_far/groundtruth.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileInterfaceException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m abs_dataset_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/groundtruths/tum_rgbd\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;66;03m# Edit this to path of dataset w.r.t. current file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ref_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(abs_dataset_path,trajectory,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundtruth.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m ref \u001b[38;5;241m=\u001b[39m \u001b[43mfile_interface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_tum_trajectory_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \n\u001b[1;32m     10\u001b[0m     est_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results/df-vo/result/tum/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrajectory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:103\u001b[0m, in \u001b[0;36mread_tum_trajectory_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_tum_trajectory_file\u001b[39m(file_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoseTrajectory3D:\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    parses trajectory file in TUM format (timestamp tx ty tz qx qy qz qw)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    :param file_path: the trajectory file path (or file handle)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    :return: trajectory.PoseTrajectory3D object\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     raw_mat \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_read_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTUM trajectory files must have 8 entries per row \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand no trailing delimiter at the end of the rows (space)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_mat \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(raw_mat) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_mat[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/olayaenv/lib/python3.10/site-packages/evo/tools/file_interface.py:84\u001b[0m, in \u001b[0;36mcsv_read_matrix\u001b[0;34m(file_path, delim, comment_str)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileInterfaceException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv file \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_path) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     85\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     skip_3_bytes \u001b[38;5;241m=\u001b[39m has_utf8_bom(file_path)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileInterfaceException\u001b[0m: csv file /home/freyja/Olaya-data/dev/monocular_visual_slam_survey/evaluation/groundtruths/tum_rgbd/rgbd_dataset_freiburg3_nostructure_notexture_far/groundtruth.txt does not exist"
     ]
    }
   ],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"TUM_RGBD\"\n",
    "trajectories = [\"rgbd_dataset_freiburg3_nostructure_notexture_far\",\"rgbd_dataset_freiburg1_360\",\"rgbd_dataset_freiburg1_rpy\", \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/tum_rgbd\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"groundtruth.txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/tum/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI dataset\n",
    "**Load KITTI files with** entries of the first three rows of  SE(3)  matrices per line (no timestamps)\n",
    "DFVO records the dataset in KITTI format but with indexes, uncompatible with EVO. Therefore we create a temp file with valid format to load trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"KITTI\"\n",
    "algorithm = \"DFVO\"\n",
    "trajectories = [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\",\"08\", \"09\", \"10\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/kitti/data_odometry_poses/dataset/poses\"# Edit this to path of dataset w.r.t. current file\n",
    "\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory+\".txt\")\n",
    "    timestamp_file =os.getcwd()+\"/groundtruths/kitti/data_odometry_calib/dataset/sequences/\"+trajectory+\"/times.txt\"\n",
    "    ref = kitti_poses_and_timestamps_to_trajectory(ref_file, timestamp_file)\n",
    "    try:    \n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/kitti/\"+trajectory+\"/\"+trajectory+\".txt\"\n",
    "\n",
    "        # create temporary file\n",
    "        temp_est_file = pd.read_csv(est_file, header = None, sep=\" \", index_col=0)\n",
    "        temp_est_file.to_csv(\"temp.txt\",sep=\" \",index=False, header= False)\n",
    "\n",
    "\n",
    "        est_fixed = kitti_poses_and_timestamps_to_trajectory(\"temp.txt\", timestamp_file) \n",
    "\n",
    "        df = metrics_dataframe(ref,est_fixed,algorithm,dataset,trajectory)    \n",
    "        #delete temporary file\n",
    "        os.remove(\"temp.txt\")\n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQUALOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"AQUALOC\"\n",
    "trajectories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "for trajectory in trajectories:\n",
    "    trajectory_folder = \"archaeo_sequence_\"+trajectory+\"_raw_data\"\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/aqualoc\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,\"Archaeological_site_sequences\",\"colmap_traj_sequence_\"+trajectory+\".txt\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)      \n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/aqualoc/Archaeological_site_sequences/\"+trajectory_folder+\"/\"+trajectory_folder+\".txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"DFVO\"\n",
    "dataset = \"MIMIR\"\n",
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n",
    "\n",
    "for trajectory in trajectories:\n",
    "    abs_dataset_path = os.getcwd()+\"/groundtruths/mimir\"# Edit this to path of dataset w.r.t. current file\n",
    "    ref_file = os.path.join(abs_dataset_path,trajectory,\"auv0\",\"pose_groundtruth\",\"data.tum\")\n",
    "    ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "    try:\n",
    "        est_file = os.getcwd()+\"/results/df-vo/result/mimir/\"+trajectory+\"/result.txt\"\n",
    "        est = file_interface.read_tum_trajectory_file(est_file)\n",
    "        ref.timestamps*=1e9\n",
    "        df = metrics_dataframe(ref,est,algorithm,dataset,trajectory)    \n",
    "\n",
    "        result_df = pd.concat([result_df, df], axis=\"columns\", join=\"outer\")\n",
    "    except:\n",
    "        print(\"failed at trajectory \",trajectory)\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-6b8b1023934c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-6b8b1023934c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    trajectories = [,,\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "trajectories = [\"\",\"\",\"\", \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16958469459171965"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.DFVO.MIMIR[\"OceanFloor/track0_dark\"].rpe_rad.loc[\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trajectories = [\"OceanFloor/track0_dark\",\"OceanFloor/track0_light\",\"SeaFloor/track0\",\"SeaFloor/track1\", \"SeaFloor_Algae/track0\",\"SeaFloor_Algae/track1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evo.tools import pandas_bridge\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for result in results:\n",
    "    df = pd.concat((df, pandas_bridge.result_to_df(result)), axis=\"columns\")\n",
    "df = df.T\n",
    "df.loc[(df['info'].dataset == \"TUM_RGBD\")  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(df.info.dataset == \"KITTI\") & (df.info.algorithm == \"DF-VO\")]\n",
    "df.loc[(df['info'].dataset == \"TUM_RGBD\") & (df['info'].algorithm == \"ORB_SLAM3_mono\") & (df['info'].trajectory == \"rgbd_dataset_freiburg3_nostructure_texture_near_withloop\")][\"stats\"][\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['info'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "state": {
    "54cc6cb1d20f45438fb3663d98d29406": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "88c180f9f59147a592d738936cecf614": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
